URL: https://docs.mparticle.com/developers/apis/warehouse-sync-api/quickstart/

DOCSDOCSHomeGuidesDevelopersIntegrationsChangelogSign UpDocumentationDevelopersAPI ReferencesPlatform APIPlatform API OverviewAccountsAppsAudiencesCalculated AttributesData PointsFeedsField TransformationsServicesUsersWorkspacesData Subject Request APIData Subject Request API Version 1 and 2Data Subject Request API Version 3Warehouse Sync APIWarehouse Sync API OverviewWarehouse Sync API TutorialWarehouse Sync API ReferenceData MappingWarehouse Sync SQL ReferenceWarehouse Sync Troubleshooting GuideComposeIDWarehouse Sync API v2 MigrationCalculated Attributes Seeding APIBulk Profile Deletion API ReferenceCustom Access Roles APIData Planning APIGroup Identity API ReferencePixel ServiceProfile APIEvents APImParticle JSON Schema ReferenceIDSyncClient SDKsAMPAMP SDKAndroidInitializationConfigurationNetwork Security ConfigurationEvent TrackingUser AttributesIDSyncScreen EventsCommerce EventsLocation TrackingMediaKitsApplication State and Session ManagementData Privacy ControlsError TrackingOpt OutPush NotificationsWebView IntegrationLoggerPreventing Blocked HTTP Traffic with CNAMELinting Data PlansTroubleshooting the Android SDKAPI ReferenceUpgrade to Version 5CordovaCordova PluginIdentityDirect Url RoutingDirect URL Routing FAQWebAndroidiOSFlutterGetting StartedUsageAPI ReferenceiOSInitializationConfigurationEvent TrackingUser AttributesIDSyncScreen TrackingCommerce EventsLocation TrackingMediaKitsApplication State and Session ManagementData Privacy ControlsError TrackingOpt OutPush NotificationsWebview IntegrationUpload FrequencyApp ExtensionsPreventing Blocked HTTP Traffic with CNAMELinting Data PlansTroubleshooting iOS SDKSocial NetworksiOS 14 GuideiOS 15 FAQiOS 16 FAQiOS 17 FAQiOS 18 FAQAPI ReferenceUpgrade to Version 7RokuGetting StartedIdentityMediaReact NativeGetting StartedIdentityUnityUpload FrequencyGetting StartedOpt OutInitialize the SDKEvent TrackingCommerce TrackingError TrackingScreen TrackingIdentityLocation TrackingSession ManagementXboxGetting StartedIdentityWebInitializationConfigurationContent Security PolicyEvent TrackingUser AttributesIDSyncPage View TrackingCommerce EventsLocation TrackingMediaKitsApplication State and Session ManagementData Privacy ControlsError TrackingOpt OutCustom LoggerPersistenceNative Web ViewsSelf-HostingMultiple InstancesWeb SDK via Google Tag ManagerPreventing Blocked HTTP Traffic with CNAMEFacebook Instant ArticlesTroubleshooting the Web SDKBrowser CompatibilityLinting Data PlansAPI ReferenceUpgrade to Version 2 of the SDKXamarinGetting StartedIdentityWebAlexaMedia SDKsAndroidiOSWebToolsmParticle Command Line InterfaceLinting ToolsSmartypeServer SDKsNode SDKGo SDKPython SDKRuby SDKJava SDKQuickstartAndroidOverviewStep 1. Create an inputStep 2. Verify your inputStep 3. Set up your outputStep 4. Create a connectionStep 5. Verify your connectionStep 6. Track eventsStep 7. Track user dataStep 8. Create a data planStep 9. Test your local appHTTP Quick StartStep 1. Create an inputStep 2. Create an outputStep 3. Verify outputiOS Quick StartOverviewStep 1. Create an inputStep 2. Verify your inputStep 3. Set up your outputStep 4. Create a connectionStep 5. Verify your connectionStep 6. Track eventsStep 7. Track user dataStep 8. Create a data planJava Quick StartStep 1. Create an inputStep 2. Create an outputStep 3. Verify outputNode Quick StartStep 1. Create an inputStep 2. Create an outputStep 3. Verify outputPython Quick StartStep 1. Create an inputStep 2. Create an outputStep 3. Verify outputWebOverviewStep 1. Create an inputStep 2. Verify your inputStep 3. Set up your outputStep 4. Create a connectionStep 5. Verify your connectionStep 6. Track eventsStep 7. Track user dataStep 8. Create a data planGuidesPartnersIntroductionOutbound IntegrationsOutbound IntegrationsFirehose Java SDKInbound IntegrationsKit IntegrationsOverviewAndroid Kit IntegrationJavaScript Kit IntegrationiOS Kit IntegrationCompose IDData Hosting LocationsGlossaryMigrate from Segment to mParticleMigrate from Segment to mParticleMigrate from Segment to Client-side mParticleMigrate from Segment to Server-side mParticleSegment-to-mParticle Migration ReferenceRules Developer GuideAPI Credential ManagementThe Developer's Guided Journey to mParticleGuidesGetting StartedCreate an InputStart capturing dataConnect an Event OutputCreate an AudienceConnect an Audience OutputTransform and Enhance Your DataPersonalizationIntroductionProfilesAudiencesAudiences OverviewCreate an AudienceConnect an AudienceManage AudiencesReal-time Audiences (Legacy)Standard Audiences (Legacy)Calculated AttributesCalculated Attributes OverviewUsing Calculated AttributesCreate with AI AssistanceCalculated Attributes ReferencePredictive AudiencesPredictive Audiences OverviewUsing Predictive AudiencesJourneysJourneys OverviewManage JourneysDownload an audience from a journeyAudience A/B testing from a journeyJourneys 2.0Predictive AttributesWhat are predictive attributes?Predict Future BehaviorCreate Future PredictionUse Future Predictions in CampaignsAssess and Troubleshoot PredictionsNext Best ActionNext Best Action OverviewCreate a Next Best Action (NBA)View and Manage NBAsActivate Next Best Actions in CampaignsPlatform GuideBillingUsage and Billing ReportThe New mParticle ExperienceThe new mParticle ExperienceThe Overview MapObservabilityObservability OverviewObservability User GuideObservability Troubleshooting ExamplesObservability Span GlossaryIntroductionData RetentionConnectionsActivityLive StreamData FilterRulesTiered EventsmParticle Users and RolesAnalytics Free TrialTroubleshooting mParticleUsage metering for value-based pricing (VBP)AnalyticsIntroductionSetupSync and Activate Analytics User Segments in mParticleUser Segment ActivationWelcome Page AnnouncementsSettingsProject SettingsRoles and TeammatesOrganization SettingsGlobal Project FiltersPortfolio AnalyticsAnalytics Data ManagerAnalytics Data Manager OverviewEventsEvent PropertiesUser PropertiesRevenue MappingExport DataUTM GuideQuery BuilderData DictionaryQuery Builder OverviewModify Filters With And/Or ClausesQuery-time SamplingQuery NotesFilter Where ClausesEvent vs. User PropertiesGroup By ClausesAnnotationsCross-tool CompatibilityApply All for Filter Where ClausesDate Range and Time Settings OverviewUser Attributes at Event TimeUnderstanding the Screen View EventAnalysesAnalyses IntroductionSegmentation: BasicsGetting StartedVisualization OptionsFor ClausesDate Range and Time SettingsCalculatorNumerical SettingsSegmentation: AdvancedAssisted AnalysisProperties ExplorerFrequency in SegmentationTrends in SegmentationDid [not] Perform ClausesCumulative vs. Non-Cumulative Analysis in SegmentationTotal Count of vs. Users Who PerformedSave Your Segmentation AnalysisExport Results in SegmentationExplore Users from SegmentationFunnels: BasicsGetting Started with FunnelsGroup By SettingsConversion WindowTracking PropertiesDate Range and Time SettingsVisualization OptionsInterpreting a Funnel AnalysisFunnels: AdvancedGroup ByFiltersConversion over TimeConversion OrderTrendsFunnel DirectionMulti-path FunnelsAnalyze as Cohort from FunnelSave a Funnel AnalysisExplore Users from a FunnelExport Results from a FunnelCohortsGetting Started with CohortsAnalysis ModesSave a Cohort AnalysisExport ResultsExplore UsersSaved AnalysesManage Analyses in DashboardsJourneysGetting StartedEvent MenuVisualizationEnding EventSave a Journey AnalysisUsersGetting StartedUser Activity TimelinesTime SettingsExport ResultsSave A User AnalysisDashboardsDashboards––Getting StartedManage DashboardsDashboard FiltersOrganize DashboardsScheduled ReportsFavoritesTime and Interval Settings in DashboardsQuery Notes in DashboardsUser AliasingAnalytics ResourcesThe Demo EnvironmentKeyboard ShortcutsTutorialsAnalytics for MarketersAnalytics for Product ManagersCompare Conversion Across Acquisition SourcesAnalyze Product Feature UsageIdentify Points of User FrictionTime-based Subscription AnalysisDashboard Tips and TricksUnderstand Product StickinessOptimize User Flow with A/B TestingUser SegmentsAPIsUser Segments Export APIDashboard Filter APIIDSyncIDSync OverviewUse Cases for IDSyncComponents of IDSyncStore and Organize User DataIdentify UsersDefault IDSync ConfigurationProfile Conversion StrategyProfile Link StrategyProfile Isolation StrategyBest Match StrategyAliasingData MasterGroup IdentityOverviewCreate and Manage Group DefinitionsIntroductionCatalogLive StreamData PlansData PlansBlocked Data Backfill GuideWarehouse SyncData Privacy ControlsData Subject RequestsDefault Service LimitsFeedsCross-Account Audience SharingApproved Sub-ProcessorsImport Data with CSV FilesImport Data with CSV FilesCSV File ReferenceGlossaryVideo IndexAnalytics (Deprecated)Identity ProvidersSingle Sign-On (SSO)Setup ExamplesSettingsDebug ConsoleData Warehouse Delay AlertingIntroductionDeveloper DocsIntroductionIntegrationsIntroductionRudderstackGoogle Tag ManagerSegmentData Warehouses and Data LakesAdvanced Data Warehouse SettingsAWS Kinesis (Snowplow)AWS Redshift (Define Your Own Schema)AWS S3 Integration (Define Your Own Schema)AWS S3 (Snowplow Schema)BigQuery (Snowplow Schema)BigQuery Firebase SchemaBigQuery (Define Your Own Schema)GCP BigQuery ExportSnowflake (Snowplow Schema)Snowplow Schema OverviewSnowflake (Define Your Own Schema)APIsREST APIDashboard Filter API (Deprecated)User Segments Export API (Deprecated)SDKsSDKs IntroductionReact NativeiOSAndroidJavaJavaScriptPythonObject APIDeveloper BasicsAliasingWarehouse Sync API TutorialUse this tutorial to configure your first Warehouse Sync pipeline using the mParticle Postman collection, and use the data from your pipeline to create an mParticle Audience. Postman is an easy and friendly environment for both developers and non-developers to use APIs.
This tutorial is not a complete guide to all Warehouse Sync features. For a complete API reference, see the Warehouse Sync API Reference.
Warehouse Sync API supports ingesting data from Snowflake, Google BigQuery, Amazon Redshift, and Databricks.
This guide explains how to ingest data into mParticle from an external data warehouse. If you want to forward data from mParticle to your data warehouse, refer to the outbound integration documentation for Amazon Redshift, Google BigQuery, or Snowflake.
You must have an mParticle user role with either "Admin", "Admin & Compliance", or "Support Access" permissions to create and modify Warehouse Sync configurations. Custom Access Rolesdoes not currently support Warehouse Sync inbound integrations.
Prerequisites
Install the latest version of the Postman desktop application. You can download Postman from https://www.postman.com/downloads/.
Fork the mParticle Warehouse Sync Postman Collection to your workspace:
A copy of the Warehouse Sync environment is included. You can download it again here.
Step 1. mParticle setup
Create Platform API credentials
You need credentials to use the Platform API to create Warehouse Sync pipelines.
To create a Platform API credential:
After signing in to the mParticle app as a user with the Admin role, click the gear icon in the bottom left corner.
Click Platform.
Select the API Credentials tab.
Click the green Add Credential button in the top right.
Give the credential a name, check the Platform checkbox and select Admin from the Permissions dropdown menu. Click the green Save button.
Copy the Client ID and Client Secret values for use in a later step.
Click Done.
Step 2. Data warehouse setup
Work with your warehouse administrator or IT team to ensure your warehouse is reachable and accessible by mParticle.
Whitelist the mParticle IP address range so your warehouse will be able to accept inbound API requests from mParticle.
Ask your database administrator to perform the following steps in your warehouse to create a new role that mParticle can use to access your database. Select the correct tab for your warehouse (Snowflake, Google BigQuery, Amazon Redshift, or Databricks) below.
Snowflake
Run the following commands from your Snowflake instance:
Placeholders in the following examples are indicated by {{Curly Braces}}. Replace these with values based on your environment and warehouse. Fields starting with mp_ are provided by mParticle.
Be very careful when entering your environment values! Simple mistakes, typos, or incorrect casing can prevent Warehouse Sync from working.
USE ROLE ACCOUNTADMIN;
// mParticle recommends creating a unique role for warehouse sync
CREATE ROLE IF NOT EXISTS {{role_name}};
GRANT USAGE ON WAREHOUSE {{warehouse}} TO ROLE {{role_name}};
GRANT USAGE ON DATABASE {{database}} TO ROLE {{role_name}};
GRANT USAGE ON SCHEMA {{database}}.{{schema}} TO ROLE {{role_name}};
// Grant SELECT privilege on any tables/views mP needs to access
GRANT SELECT ON TABLE {{database}}.{{schema}}.{{table}} TO ROLE {{role_name}};
// mParticle recommends creating a unique user for mParticle
// Mark your new user as a legacy service user to exclude it from Snowflake's multifactor authentication policy
CREATE OR REPLACE USER {{user_name}} PASSWORD = "{{unique_secure_password}}" TYPE = LEGACY_SERVICE;
GRANT ROLE {{role_name}} TO USER {{user_name}};
CREATE OR REPLACE STORAGE INTEGRATION {{storage_integration_name}}
WITH TYPE = EXTERNAL_STAGE
STORAGE_PROVIDER = S3
ENABLED = TRUE
STORAGE_AWS_ROLE_ARN = "arn:aws:iam::{{mp_pod_aws_account_id}}:role/ingest-pipeline-data-external-{{mp_org_id}}-{{mp_acct_id}}"
STORAGE_AWS_OBJECT_ACL = "bucket-owner-full-control"
STORAGE_ALLOWED_LOCATIONS = ("s3://{{mp_pod}}-ingest-pipeline-data/{{mp_org_id}}/{{mp_acct_id}}");
GRANT USAGE ON INTEGRATION {{storage_integration_name}} TO ROLE {{role_name}};
Where:
role_name: The ID of the role mParticle will assume while executing SQL commands on your Snowflake instance. mParticle recommends creating a unique role for warehouse sync.
warehouse: The ID of the Snowflake virtual warehouse compute cluster where SQL commands will be executed.
database: The ID of the database in your Snowflake instance from which you want to sync data.
schema: The ID of the schema in your Snowflake instance containing the tables you want to sync data from.
table: The ID of the table containing data you want to sync. Grant SELECT privileges on any tables/views mParticle needs to access.
user_name: The ID of the user mParticle will log in as while executing SQL commands on your Snowflake instance. mParticle recommends creating a unique role for warehouse sync.
If creating a new user, use a new unique_secure_password.
storage_integration_name: The ID of a Snowflake external storage integration allowing mParticle to unload data from your Snowflake instance to an S3 storage bucket.
mp_pod: The mParticle region ID of your data hosting location, one of US1, US2, AU1, or EU1.
mp_pod_aws_account_id: The mParticle provided ID for the data hosting location where your organization resides. Use the corresponding value for your mParticle instance:
US1: 338661164609
US2: 386705975570
AU1: 526464060896
EU1: 583371261087
mp_org_id: The mParticle provided ID of your organization where this connection will be stored. This can be found from your API client setup in step 1.
mp_acct_id: The mParticle provided ID of the account where this connection will be stored. This can be found from your API client setup in step 1.
Beginning September 30th, 2024, Snowflake is enforcing Multi-factor authentication (MFA) for all human users (as opposed to service users) that authenticate with a username and password in new Snowflake accounts, created after Snowflake's BCR 2024_08 bundle release. Service user accounts, which are often accounts designed for system-to-system or application-to-application communication, are exempted from Snowflake's MFA policy.
To prevent issues with your Snowflake configuration, make sure to set your new Snowflake user type as legacy service to exclude it from Snowflake's MFA policy by adding TYPE = LEGACY_SERVICE to the CREATE USER statement in your SQL statement. Alternatively, you can run the ALTER USER {{user_name}} SET TYPE = LEGACY_SERVICE; SQL statement to modify existing users.
Google BigQuery
The Google BigQuery connector uses the enterprise grade Google Cloud Platform infrastructure. Safeguarding customer data is our highest priority, and Data Warehouse Sync employs the same SOC2 best practices as the rest of the mParticle platform. mParticle expects to include Data Warehouse Sync in the upcoming annual SOC2 audit.
Create a new service account for mParticle
Go to console.cloud.google.com, log in, and navigate to IAM & Admin > Service Accounts.
Select Create Service Account.
Enter a new identifier for mParticle in Service account ID. In the example below, the email address is the service account ID. Save this value for your Postman setup.
Under Grant this service account access to project, select BigQuery Job User under the Role dropdown menu, and click DONE.
Select your new service account and navigate to the Keys tab.
Click ADD KEY and select Create new key. The value for service_account_key will be the contents of the generated JSON file. Save this value for your Postman setup.
Identify your BigQuery warehouse details
Navigate to your BigQuery instance from console.cloud.google.com.
Your project_id is the first portion of Dataset ID (the portion before the .). In the example above, it is mp-project.
Your dataset_id is the second portion of Dataset ID (the portion immediately after the .) In the example above, it is mp-dataset.
Your region is the Data location. This is us-east4 in the example above.
Grant access to the dataset in BigQuery
From your BigQuery instance in console.cloud.google.com, click Sharing and select Permissions.
Click Add Principle.
Assign two Roles, one for BigQuery Data Viewer, and one for BigQuery User.
Click Save.
Amazon Redshift
Navigate to your AWS Console, log in with your administrator account, and navigate to your Redshift cluster details.
Run the following SQL statements to create a new user for mParticle, grant the necessary schema permissions to the new user, and grant the necessary access to your tables/views.
-- Create a unique user for mParticle
CREATE USER {{user_name}} WITH PASSWORD '{{unique_secure_password}}'
-- Grant schema usage permissions to the new user
GRANT USAGE ON SCHEMA {{schema_name}} TO {{user_name}}
-- Grant SELECT privilege on any tables/views mP needs to access to the new user
GRANT SELECT ON TABLE {{schema_name}}.{{table_name}} TO {{user_name}}
Navigate to the Identity And Access Management (IAM) dashboard, select Roles under the left hand nav bar, and click Create role.
In Step 1 Select trusted entity, click AWS service under Trusted entity
Select Redshift from the dropdown menu titled “Use cases for other AWS services”, and select Redshift - Customizable. Click Next.
In Step 2 Add permissions, click Create Policy.
Click JSON in the policy editor, and enter the following permissions before clicking Next.
Replace {mp_pod_aws_account_id} with one of the following values according to your mParticle instance’s location:
US1 = 338661164609
US2 = 386705975570
AU1 = 526464060896
EU1 = 583371261087
{
"Statement": [
{
"Action": "sts:AssumeRole",
"Effect": "Allow",
"Resource": "arn:aws:iam::{{mp_pod_aws_account_id}}:role/ingest-pipeline-data-external-{{mp_org_id}}-{{mp_acct_id}}",
"Sid": ""
}
],
"Version": "2012-10-17"
}
Enter a meaningful name for your new policy, such as mparticle_redshift_assume_role_policy, and click Create policy.
Return to the Create role tab, click the refresh button, and select your new policy. Click Next.
Enter a meaningful name for your new role, such as mparticle_redshift_role, and click Create role.
Your configuration will differ between Amazon Redshift and Amazon Redshift Serverless. To complete your configuration, follow the appropriate steps for your use case below.
Make sure to save the value of your new role’s ARN. You will need to use this when setting up Postman in the next section.
Amazon Redshift (not serverless)
Navigate to your AWS Console, then navigate to Redshift cluster details. Select the Properties tab.
Scroll to Associated IAM roles, and select Associate IAM Roles from the Manage IAM roles dropdown menu.
Select the new role you just created. The name for the role in this example is mparticle_redshift_role.
Amazon Redshift Serverless
Navigate to your AWS Console, then navigate to your Redshift namespace configuration. Select the Security & Encryption tab, and click Manage IAM roles.
Select Associate IAM roles from the Manage IAM roles dropdown menu.
Select the new role you just created. The name for the role in this example is mparticle_redshift_role.
Databricks
Warehouse Sync uses the Databricks-to-Databricks Delta Sharing protocol to ingest data from Databricks into mParticle.
Due to a limitation with Delta Sharing, Warehouse Sync does not support ingesting views from Databricks. Learn more.
Complete the following steps to prepare your Databricks instance for Warehouse Sync.
1. Enable Delta Sharing
Log into your Databricks account and navigate to the Account Admin Console. You must have Account Admin user privileges.
Click Catalog from the left hand nav bar, and select the metastore you want to ingest data from.
Select the Configuration tab. Under Delta Sharing, check the box labeled “Allow Delta Sharing with parties outside your organization”.
Find and save your Databricks provider name: the value displayed under Organization name. You will use this value for your provider name when creating the connection between the mParticle Warehouse Sync API and Databricks.
2. Configure a Delta Sharing recipient for mParticle
From the Unity Catalog Explorer in your Databricks account, click the Delta Sharing button, and select Shared by me.
Click New Recipient in the top right corner.
Within the Create a new recipient window, enter mParticle_{YOUR-DATA-POD} under Recipient name where {YOUR-DATA-POD} is either us1, us2, eu1, or au1 depending on the location of the data pod configured for your mParticle account.
In Sharing identifier, enter one of the following identifiers below, depending on the location of your mParticle account’s data pod:
US1: aws:us-east-1:e92fd7c1-5d24-4113-b83d-07e0edbb787b
US2: aws:us-east-1:e92fd7c1-5d24-4113-b83d-07e0edbb787b
EU1: aws:eu-central-1:2b8d9413-05fe-43ce-a570-3f6bc5fc3acf
AU1: aws:ap-southeast-2:ac9a9fc4-22a2-40cc-a706-fef8a4cd554e
3. Share your Databricks tables and schema with your new Delta Sharing recipient
From the Unity Catalog Explorer in your Databricks account, click the Delta Sharing button.
Click Share data in the top right.
Within the Create share window, enter mparticle_{YOUR-MPARTICLE-ORG-ID}_{YOUR-MPARTICLE-ACCOUNT-ID} under Share name where {YOUR-MPARTICLE-ORG-ID} and {YOUR-MPARTICLE-ACCOUNT-ID} are your mParticle Org and Account IDs.
To find your Org ID, log into the mParticle app. View the page source. For example, in Google Chrome, go to View > Developer > View Page Source. In the resulting source for the page, look for “orgId”:xxx. This number is your Org ID.
Follow a similar process to find your Account ID (“accountId”:yyy) and Workspace ID (“workspaceId”:zzz).
Click Save and continue at the bottom right.
In the Add data assets section, select the assets to add to the schemas and tables you want to send to mParticle. Make sure to remember your schema name: you will need this value when configuring your Databricks feed in mParticle.
Click Save and continue at the bottom right until you reach the Add recipients step. (You can skip the Add notebooks step.)
In the Add recipients step, make sure to add the new mParticle recipient you created in Step 2.
Finally, click the Share data button at the bottom right.
Unsupported data types between mParticle and Databricks
Databricks Delta Sharing does not currently support the TIMESTAMP_NTZ data type.
Other data types that are not currently supported by the Databricks integration for Warehouse Sync (for both user and events data) include:
Intervals
If you are ingesting events data through Warehouse Sync, the following data types are unsupported:
Maps
Multi-dimensional arrays
While multi-dimensional, or nested, arrays are unsupported, you can still ingest simple arrays with events data.
Step 3. Postman setup
Once you have installed Postman, configure the collection environment settings and variables.
Update Postman environment settings
Ensure you forked the mParticle Warehouse Sync API Postman Collection, as described in the Prerequisites section of this tutorial. In Postman, click the Environments tab from the left navigation menu.
If you successfully forked the Warehouse Sync API collection, you’ll see it in the list of Environment configurations. You can rename it to something more meaningful by right-clicking on the … next to the name and choosing the Rename option.
Replace the the placeholders (replace_me) with the correct values for your environment. You must update the values under column labeled Current value.
Replace PLATFORM_API_CLIENT_ID and PLATFORM_API_CLIENT_SECRET with your new Platform API credentials.
Replace WORKSPACE_ID, ACCOUNT_ID, and ORG_ID with the corresponding values for your mParticle account.
To find your Org ID, log into the mParticle app. View the page source. For example, in Google Chrome, go to View > Developer > View Page Source. In the resulting source for the page, look for “orgId”:xxx. This number is your Org ID.
Follow a similar process to find your Account ID (“accountId”:yyy) and Workspace ID (“workspaceId”:zzz).
Replace POD with the regional pod your mParticle account is deployed on. Look at the URL in your browser where you are signed into mParticle. The POD is one of the following values: US1, US2, EU1, AU1.
Enter the data warehouse usernames and passwords you saved from “Step 2. Data Warehouse Setup.” according to the data warehouse you are using:
For Snowflake, replace SNOWFLAKE_PASSWORD and SNOWFLAKE_STORAGE_INTEGRATION with the values you saved in step 2. Please refer to the Snowflake documentation to determine your account_identifier and region.
For BigQuery, replace BIG_QUERY_SERVICE_ACCOUNT_ID with the service account ID you used in BigQuery, and BIG_QUERY_SERVICE_ACCOUNT_KEY with the key from the generated JSON file in step 2.
For Redshift, replace REDSHIFT_USER, REDSHIFT_PASSWORD, and REDSHIFT_AWS_IAM_ROLE_ARN with the values created in step 2.
After updating all necessary values, run COMMAND-S (or CTRL-S) to save your changes.
Update the Postman collection
Ensure you forked the mParticle Warehouse Sync API Postman Collection as described in the Prerequisites section. In Postman, click the Collections tab on the left hand navigation.
Once successfully forked, you’ll see the collection in the list of available collections.
Click Warehouse Sync API, then select the Variables tab.
Replace replace_me placeholders with the values corresponding to your environment. Ensure you update the values in the Current value column.
Replace INGEST_PIPELINE_SLUG and INGEST_PIPELINE_NAME with the slug and name you want to use to identify your new pipeline.
Replace SQL_QUERY with the database SQL query mParticle will use to retrieve data from your warehouse. SQL is a powerful language and you can use advanced expressions to filter, aggregate, join, etc. your data. Work with your database administrator if you need help crafting the right SQL query:
Your query can contain a timestamp column that mParticle will use to keep track of which rows need to be loaded.
Your query should contain one or more user identity columns that mParticle will use to perform identity resolution to ensure that data ends up on the correct user profile.
As part of the SQL query, you must specify how columns in the query will map to attributes on a user’s profile. You do this by using column aliasing in SQL. For example, in the following query, the column cid in Snowflake is being mapped to the mParticle attribute customer_id.
If you don’t provide an alias, mParticle will use the name of the column in your database. If an attribute of this name does not already exist on the user’s profile, mParticle will create a new attribute with this name.
Before using the query in mParticle, test the query outside of mParticle to ensure it returns the data you expect.
To learn more checkout the Warehouse Sync SQL reference
After updating all necessary values, run COMMAND-S (or CTRL-S) to save your changes.
Step 4: Create your first Warehouse Sync pipeline
Creating a Warehouse Sync pipeline requires completing the following steps:
Create a partner feed.
Create a connection.
Create a data model.
Create a field transformation
Create the pipeline.
After configuration, you can monitor the pipeline.
Create the partner feed
First, you must create a data feed by submitting a request to the Feeds API. mParticle uses this feed for rules and connections in your workspace. You must provide a value for module_name that corresponds with the data warehouse you are using.
Valid values for module_name are:
Redshift
BigQuery
Snowflake
Databricks
These values are case sensitive. For example, if you use snowflake instead of Snowflake, you will encounter errors later in your configuration.
In Postman, ensure the environment drop-down is pointed to the Environment configuration you recently imported.
Expand the Warehouse Sync Collection and open the Feeds folder.
Click Create Warehouse Sync Feed.
Click the Body tab to see the information you will pass to the API in order to create the feed.
module_name must be one of Snowflake, BigQuery, or Redshift
Values surrounded by double braces (for example: {{WORKSPACE_ID}}) are taken from the variables you updated in previous steps of this tutorial.
Verify all values you changed, and click the blue Send button. mParticle returns a success message with details about your new feed. If the request fails, mParticle returns an error message with additional information.
Create the connection
The next step is to create a connection between mParticle and your data warehouse.
In Postman, ensure the environment drop-down is pointed to the Environment configuration you recently imported.
Expand the Warehouse Sync Collection and open the Connections folder.
Click POST Create Snowflake/BigQuery/Redshift Connection, selecting the appropriate endpoint for the data warehouse you are using.
Click the Body tab and replace each "replace_me" placeholder with the correct value for your specific warehouse.
The values in {{Sample Values}} are taken from the environment variables you updated in earlier steps. Make sure these values match the values for your organization’s data warehouse. You may need to work with your database administrator to ensure you have the correct values.
Verify all values you changed, and click the blue Send button. mParticle returns a success message with details about the configuration you just created. If the request fails, mParticle returns an error message with additional information.
Create the data model
The next step is to create a data model. A data model is a SQL query that mParticle sends to your warehouse specifying exactly what columns, rows, and fields of data you want to ingest through your pipeline.
In the case of pipelines that ingest user profile data, the data model is also responsible for mapping ingested data fields to mParticle user attributes.
To create a data model using the Warehouse Sync API:
In Postman, expand Warehouse Sync API and click Data Models.
Click Create Data Model.
Select the Body tab and enter your data model.
The values in {{Sample Values}} are taken from the variables you updated in previous steps.
Once you are confident all values are correct, click the blue Send button. mParticle returns a success message with details about the data model you just created. If the request fails, mParticle returns an error message with additional information.
For more details about using SQL to create a data model, with example queries and best practices, see the Warehouse Sync SQL Reference.
Certain Warehouse Sync pipeline parameters, like iterator_field, are case-insensitive by default, so avoid the use of strict casing syntax in the SELECT statements of your SQL query as this may result in errors when creating your pipeline. For more information, see Data models and SQL queries.
Certain Warehouse Sync pipeline parameters, like iterator_field, are case-insensitive by default, so avoid the use of strict casing syntax in the SELECT statements of your SQL query as this may result in errors when creating your pipeline. For more information, see Data models and SQL queries.
Create the field transformation
This step is only required if you are ingesting events data. If you are creating a pipeline to ingest user profile data, the data model you created in the previous step sufficiently specifies how your source data will map to mParticle user attributes. To learn more about how data models map source data to user attributes, see Default mapping for user attribute pipelines.
Optionally, you can use a field transformation to map user data for more complex use cases.
You can create a field transformation to specify exactly how fields in your database should map to fields in the mParticle JSON schema.
Every column returned by your data model's SQL query must be mapped to fields in the mParticle JSON schema using a field transformation. If you want to omit or ignore certain columns, you can modify your data model's SQL query to remove them from your syncs, or you can include specific mappings in your field transformation using the ignore mapping type.
For detailed instructions on how to create a field transformation, read Event Data Mapping.
To use your field transformation, add the field_transformation_id to the request body of your API call when creating your pipeline in the next step.
Create the pipeline
The final step is to create the pipeline. You pass in the connection and data model configurations previously created along with your sync mode and scheduling settings.
In Postman, expand Warehouse Sync Collection and open the Pipelines folder.
Click Create Pipeline.
Select the Body tab, and update the sync_mode and schedule settings as follows:
For sync_mode:
Set type to either incremental or full
If you set type to incremental, Set iterator_field to the name of the column in your sql query mParticle will monitor to track changes that need to be synced
If you set type to incremental, Set iterator_data_type to the datatype of your iterator
Valid values for Snowflake are: timestamp_ltz, timestamp_ntz, timestamp_tz, datetime, date, timestamp_unixtime_ms, timestamp_unixtime_s.
Valid values for Google BigQuery are datetime, date, timestamp, timestamp_unixtime_ms, timestamp_unixtime_s.
Valid values for AWS Redshift are date, timestamp, timestamptz, timestamp_unixtime_ms, timestamp_unixtime_s.
Valid values for Databricks are date, timestamp, timestamp_unixtime_ms, timestamp_unixtime_s.
Do not select an iterator datatype that doesn't account for timezones if the timestamps in your database include timezone names or timezone offsets.
If you do select a datatype that doesn't account for timezones, you should make sure to perform any necessary timezone calculations on your timestamps before converting them to the new datatype.
For information related to your specific database provider, refer to the corresponding timestamp datatype documentation for: Snowflake, Google BigQuery, Amazon Redshift, and Databricks.
For schedule:
Set type to interval, once, or on_demand
If you set type to interval, set frequency to hourly, daily, weekly, or monthly
Set start to the date-time value you want recurring syncs to begin
<img src="/images/dwi/api-v2-tutorial/5.png" alt="Postman page showing request results">
The values in `{{Sample Values}}` will be taken from the variables you updated in previous steps. You can optionally update the `environment` variable. It is currently set to target your mParticle development environment, but you can change it to target your production environment.
Set the field_transformation_id to the ID of a a custom field transformation to map data from your warehouse to specific fields in the mParticle JSON schema. To learn more about custom field transformations and how to create them, see Field Transformations API.
You can associate an optional data plan with your pipeline to help improve the quality of, and grant you more control over, the data imported to mParticle. In the request body of API request, include the body parameters plan_id and plan_version and set these to the values of the data plan and version you want to use. You must use a data plan version that is active and exists in the mParticle workspace you are using.
Once you are confident all values are correct, click the blue Send button. If successful, mParticle returns a success message with details about the configuration you just created. If it was not successful, mParticle returns an error message with additional information.
Monitor the pipeline
Once a pipeline has been created, you can monitor its status using the additional requests provided in the Postman collection.
In Postman, expand the Warehouse Sync Collection and open the Pipelines folder.
Click Get Pipeline Report.
Click the blue Send button.
mParticle sends a detailed message with the pipeline’s current status. After creating a pipeline, there is an approximate one-minute delay until the pipeline is created in the mParticle backend, so submitting a Get Pipeline Report request results in a Not Founderror. Try again after several minutes.
While a pipeline is ingesting data, you can monitor it in mParticle as you would with any other input. From mParticle, go to Data Master > Live Stream to inspect the incoming data from Snowflake.
Once the data is ingested, the data points appear on the user’s profile. Go to Activity > User Activity and look up a sample profile. If the attributes do not appear as expected, validate the mapping you provided in the SQL query provided earlier.
Step 5. Activate the data
Now that the data has been loaded into mParticle, it’s time to put it to use by creating an audience using the newly ingested data and sending it to a downstream integration.
Create an audience
Create an audience that uses one of the attributes we ingested as a qualifying criteria for it:
In the mParticle app, go to Audiences > Standard.
Click the green New Standard Audience in the upper right corner.
Give the audience a name.
Select a date range, or choose “All available data.”
Select the Warehouse Sync feed you created in mParticle Setup.
Add an audience criteria that leverages one of the data points you ingested from your warehouse. In the example below, we only want to consider users who have a propensity-to-buy score that’s greater than 0.7.
Add any other criteria you want to be considered. Click Save As Draft when you are done.
Click Calculate to run the audience calculation.
Connect the audience to an output
In the final step, we will send users who qualified for this audience to Iterable for further targeting. If your organization doesn’t use Iterable, pick a different integration that your organization uses.
After the audience has been fully calculated, connect it to an output:
If you aren’t still there, go to Audiences > Standard.
In the row for the audience you just created, click the Connect button in the Actions column.
Click Connect Output.
Click the Add Output green button.
Click the Iterable tile in the integrations directory, or pick another integration your organization uses if you don’t have Iterable.
Select the Audience checkbox and click Configure.
Enter a configuration name, your Iterable API key, and the user ID that is used to identify users, and then click Save & Open in Connections.
Provide the ID of the list in Iterable that the user data should be loaded into, and click Add Connection.
Click the Send button to send the audience to Iterable.
Data starts flowing to Iterable.
Now you can open the audience in Iterable.Was this page helpful?YesNoLast Updated: February 27, 2025© 2025 mParticle, Inc. All rights reserved.mParticle.comPrivacy PolicyCookie PolicyDo Not Sell or Share My Personal Data